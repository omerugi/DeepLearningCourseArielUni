{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import data and split to train/test , x/y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 24 classes,the class that has the most data is class 17 with 4.713 precents of the data\n",
      "if the data were equals between all classes it needed to be 4.167 precents of the data\n"
     ]
    }
   ],
   "source": [
    "#there are 24 letters but for the order j get an index\n",
    "num_categories=25 \n",
    "\n",
    "# Importing the data - test and train\n",
    "test = pd.read_csv(\"sign_mnist_test.csv\").to_numpy() \n",
    "train = pd.read_csv(\"sign_mnist_train.csv\").to_numpy()\n",
    "\n",
    "# Splitting test into x & y - x = fetures y = labels \n",
    "# Note: we do OneHot on the y to get it as a vector at the size of the categories\n",
    "# then put's 1 where the right label is and zero in the rest\n",
    "y_test=test[:,0]\n",
    "y_test=keras.utils.to_categorical(y_test, num_categories)\n",
    "x_test=test[:,1:]\n",
    "\n",
    "# Splitting data into x & y - x = fetures y = labels\n",
    "# Note: we do OneHot on the y to get it as a vector at the size of the categories\n",
    "# then put's 1 where the right label is and zero in the rest\n",
    "data_y=train[:,0]\n",
    "data_y=keras.utils.to_categorical(data_y, num_categories)\n",
    "data_x=train[:,1:]\n",
    "\n",
    "# Getting the number of feturs\n",
    "# Note: in the case of image each pixel and the toal number is n*n\n",
    "lines,features=data_x.shape\n",
    "\n",
    "biggestClass=np.argmax(np.bincount(train[:,0]))\n",
    "precentOfDataOfBiggestClass=np.max(np.bincount(train[:,0]))*100/lines\n",
    "\n",
    "print(\"there are 24 classes,the class that has the most data is class %2d with %.3f precents of the data\"\n",
    "      %(biggestClass,precentOfDataOfBiggestClass))\n",
    "print(\"if the data were equals between all classes it needed to be %.3f precents of the data\" %(100/24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A \"list like\" object that will hold the calculatins as tensors\n",
    "# Size of x: (?, 784)\n",
    "x = tf.placeholder(tf.float32, [None, features])\n",
    "# Size of y_: (?, 25)\n",
    "y_ = tf.placeholder(tf.float32, [None, num_categories])\n",
    "\n",
    "\n",
    "W = tf.Variable(tf.zeros([features,num_categories]))\n",
    "b = tf.Variable(tf.zeros([num_categories]))\n",
    "\n",
    "# Define activation function - Softmax \n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "# Define the loss function - Cross entropy\n",
    "loss = -tf.reduce_mean(y_*tf.log(y))\n",
    "\n",
    "# Define the update function - Gradient descent\n",
    "update = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)\n",
    "\n",
    "# Define accuracy - Percentage of predictions did we get right\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1),tf.argmax(y_, 1)) \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27455, 784)\n",
      "(27455, 25)\n"
     ]
    }
   ],
   "source": [
    "print(data_x.shape)\n",
    "print(data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 784)\n",
      "(?, 25)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'PrintV2_14' type=PrintV2>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.print(W, output_stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 train_acc: 0.047 test_acc: 0.020\n",
      "epoch:   1 train_acc: 0.190 test_acc: 0.253\n",
      "epoch:   2 train_acc: 0.047 test_acc: 0.020\n",
      "epoch:   3 train_acc: 0.190 test_acc: 0.199\n",
      "epoch:   4 train_acc: 0.087 test_acc: 0.051\n",
      "epoch:   5 train_acc: 0.125 test_acc: 0.094\n",
      "epoch:   6 train_acc: 0.087 test_acc: 0.050\n",
      "epoch:   7 train_acc: 0.144 test_acc: 0.130\n",
      "epoch:   8 train_acc: 0.084 test_acc: 0.061\n",
      "epoch:   9 train_acc: 0.182 test_acc: 0.179\n",
      "epoch:  10 train_acc: 0.106 test_acc: 0.086\n",
      "epoch:  11 train_acc: 0.090 test_acc: 0.086\n",
      "epoch:  12 train_acc: 0.111 test_acc: 0.048\n",
      "epoch:  13 train_acc: 0.143 test_acc: 0.135\n",
      "epoch:  14 train_acc: 0.118 test_acc: 0.116\n",
      "epoch:  15 train_acc: 0.223 test_acc: 0.184\n",
      "epoch:  16 train_acc: 0.102 test_acc: 0.078\n",
      "epoch:  17 train_acc: 0.166 test_acc: 0.152\n",
      "epoch:  18 train_acc: 0.139 test_acc: 0.103\n",
      "epoch:  19 train_acc: 0.157 test_acc: 0.109\n",
      "epoch:  20 train_acc: 0.228 test_acc: 0.187\n",
      "epoch:  21 train_acc: 0.137 test_acc: 0.152\n",
      "epoch:  22 train_acc: 0.198 test_acc: 0.148\n",
      "epoch:  23 train_acc: 0.187 test_acc: 0.176\n",
      "epoch:  24 train_acc: 0.187 test_acc: 0.143\n",
      "epoch:  25 train_acc: 0.261 test_acc: 0.233\n",
      "epoch:  26 train_acc: 0.186 test_acc: 0.195\n",
      "epoch:  27 train_acc: 0.247 test_acc: 0.240\n",
      "epoch:  28 train_acc: 0.161 test_acc: 0.097\n",
      "epoch:  29 train_acc: 0.242 test_acc: 0.258\n",
      "epoch:  30 train_acc: 0.211 test_acc: 0.191\n",
      "epoch:  31 train_acc: 0.213 test_acc: 0.187\n",
      "epoch:  32 train_acc: 0.196 test_acc: 0.214\n",
      "epoch:  33 train_acc: 0.211 test_acc: 0.153\n",
      "epoch:  34 train_acc: 0.244 test_acc: 0.220\n",
      "epoch:  35 train_acc: 0.225 test_acc: 0.187\n",
      "epoch:  36 train_acc: 0.256 test_acc: 0.247\n",
      "epoch:  37 train_acc: 0.206 test_acc: 0.169\n",
      "epoch:  38 train_acc: 0.249 test_acc: 0.226\n",
      "epoch:  39 train_acc: 0.229 test_acc: 0.238\n",
      "epoch:  40 train_acc: 0.281 test_acc: 0.261\n",
      "epoch:  41 train_acc: 0.239 test_acc: 0.173\n",
      "epoch:  42 train_acc: 0.268 test_acc: 0.276\n",
      "epoch:  43 train_acc: 0.241 test_acc: 0.168\n",
      "epoch:  44 train_acc: 0.278 test_acc: 0.239\n",
      "epoch:  45 train_acc: 0.228 test_acc: 0.254\n",
      "epoch:  46 train_acc: 0.296 test_acc: 0.247\n",
      "epoch:  47 train_acc: 0.257 test_acc: 0.238\n",
      "epoch:  48 train_acc: 0.241 test_acc: 0.220\n",
      "epoch:  49 train_acc: 0.259 test_acc: 0.185\n"
     ]
    }
   ],
   "source": [
    "# Define a session\n",
    "sess = tf.Session()\n",
    "# Init all the vaeiables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(0,50):\n",
    "    sess.run(update, feed_dict = {x:data_x, y_:data_y}) #BGD \n",
    "    train_acc = sess.run(accuracy, feed_dict={x:data_x, y_:data_y})\n",
    "    test_acc = sess.run(accuracy, feed_dict={x:x_test, y_:y_test})\n",
    "    print (\"epoch: %3d train_acc: %.3f test_acc: %.3f\" % (epoch,train_acc, test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "(hidden1_size, hidden2_size) = (100, 50)\n",
    "\n",
    "# Set the weight of the first layer - 784*100 \n",
    "# The values to be normalized [0.1,1]\n",
    "W1 = tf.Variable(tf.truncated_normal([features, hidden1_size], stddev=0.1))\n",
    "\n",
    "# Set the bais of the first layer - 100*1\n",
    "# The values - 0.1\n",
    "b1 = tf.Variable(tf.constant(0.1, shape=[hidden1_size]))\n",
    "\n",
    "# Set the output of the first layer using ReLU\n",
    "# Output size: [?,100]\n",
    "z1 = tf.nn.relu(tf.matmul(x,W1)+b1)\n",
    "\n",
    "# Set the weight of the second layer - 100*50 \n",
    "# The values to be normalized [0.1,1]\n",
    "W2 = tf.Variable(tf.truncated_normal([hidden1_size, hidden2_size], stddev=0.1))\n",
    "\n",
    "# Set the bais of the second layer - 50*1\n",
    "# The values - 0.1\n",
    "b2 = tf.Variable(tf.constant(0.1, shape=[hidden2_size]))\n",
    "\n",
    "# Set the output of the second layer using ReLU\n",
    "# Output size: [?,50]\n",
    "z2 = tf.nn.relu(tf.matmul(z1,W2)+b2)\n",
    "\n",
    "# Set the weight of the output layer - 50*25\n",
    "# The values to be normalized [0.1,1]\n",
    "W3 = tf.Variable(tf.truncated_normal([hidden2_size, num_categories], stddev=0.1))\n",
    "\n",
    "# Set the bais of the output layer - 25*1\n",
    "# The values - 0.1\n",
    "b3 = tf.Variable(tf.constant(0.1, shape=[num_categories]))\n",
    "\n",
    "# Set the output using SoftMax\n",
    "# Output size: [?,25]\n",
    "y = tf.nn.softmax(tf.matmul(z2, W3) + b3)\n",
    "\n",
    "# Define the loss function - Cross entropy\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "# Define the update function - Gradient descent\n",
    "update = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 train_acc: 0.047 test_acc: 0.020\n",
      "epoch:  10 train_acc: 0.109 test_acc: 0.090\n",
      "epoch:  20 train_acc: 0.142 test_acc: 0.147\n",
      "epoch:  30 train_acc: 0.232 test_acc: 0.185\n",
      "epoch:  40 train_acc: 0.275 test_acc: 0.261\n",
      "epoch:  50 train_acc: 0.298 test_acc: 0.254\n",
      "epoch:  60 train_acc: 0.297 test_acc: 0.229\n",
      "epoch:  70 train_acc: 0.310 test_acc: 0.256\n",
      "epoch:  80 train_acc: 0.330 test_acc: 0.270\n",
      "epoch:  90 train_acc: 0.345 test_acc: 0.283\n",
      "epoch: 100 train_acc: 0.360 test_acc: 0.312\n",
      "epoch: 110 train_acc: 0.372 test_acc: 0.319\n",
      "epoch: 120 train_acc: 0.395 test_acc: 0.342\n",
      "epoch: 130 train_acc: 0.403 test_acc: 0.339\n",
      "epoch: 140 train_acc: 0.421 test_acc: 0.358\n",
      "epoch: 150 train_acc: 0.425 test_acc: 0.358\n",
      "epoch: 160 train_acc: 0.440 test_acc: 0.370\n",
      "epoch: 170 train_acc: 0.446 test_acc: 0.377\n",
      "epoch: 180 train_acc: 0.455 test_acc: 0.381\n",
      "epoch: 190 train_acc: 0.457 test_acc: 0.375\n",
      "epoch: 200 train_acc: 0.463 test_acc: 0.384\n",
      "epoch: 210 train_acc: 0.469 test_acc: 0.388\n",
      "epoch: 220 train_acc: 0.475 test_acc: 0.393\n",
      "epoch: 230 train_acc: 0.482 test_acc: 0.399\n",
      "epoch: 240 train_acc: 0.486 test_acc: 0.404\n",
      "epoch: 250 train_acc: 0.489 test_acc: 0.402\n",
      "epoch: 260 train_acc: 0.495 test_acc: 0.405\n",
      "epoch: 270 train_acc: 0.496 test_acc: 0.403\n",
      "epoch: 280 train_acc: 0.501 test_acc: 0.405\n",
      "epoch: 290 train_acc: 0.504 test_acc: 0.406\n",
      "epoch: 300 train_acc: 0.508 test_acc: 0.409\n",
      "epoch: 310 train_acc: 0.511 test_acc: 0.410\n",
      "epoch: 320 train_acc: 0.516 test_acc: 0.410\n",
      "epoch: 330 train_acc: 0.520 test_acc: 0.411\n",
      "epoch: 340 train_acc: 0.524 test_acc: 0.411\n",
      "epoch: 350 train_acc: 0.528 test_acc: 0.412\n",
      "epoch: 360 train_acc: 0.532 test_acc: 0.412\n",
      "epoch: 370 train_acc: 0.535 test_acc: 0.412\n",
      "epoch: 380 train_acc: 0.538 test_acc: 0.411\n",
      "epoch: 390 train_acc: 0.541 test_acc: 0.410\n",
      "epoch: 400 train_acc: 0.543 test_acc: 0.409\n",
      "epoch: 410 train_acc: 0.546 test_acc: 0.409\n",
      "epoch: 420 train_acc: 0.549 test_acc: 0.408\n",
      "epoch: 430 train_acc: 0.551 test_acc: 0.409\n",
      "epoch: 440 train_acc: 0.554 test_acc: 0.408\n",
      "epoch: 450 train_acc: 0.557 test_acc: 0.408\n",
      "epoch: 460 train_acc: 0.559 test_acc: 0.408\n",
      "epoch: 470 train_acc: 0.561 test_acc: 0.407\n",
      "epoch: 480 train_acc: 0.564 test_acc: 0.409\n",
      "epoch: 490 train_acc: 0.566 test_acc: 0.411\n",
      "epoch: 500 train_acc: 0.568 test_acc: 0.413\n",
      "epoch: 510 train_acc: 0.570 test_acc: 0.415\n",
      "epoch: 520 train_acc: 0.572 test_acc: 0.421\n",
      "epoch: 530 train_acc: 0.575 test_acc: 0.422\n",
      "epoch: 540 train_acc: 0.577 test_acc: 0.425\n",
      "epoch: 550 train_acc: 0.579 test_acc: 0.426\n",
      "epoch: 560 train_acc: 0.580 test_acc: 0.426\n",
      "epoch: 570 train_acc: 0.582 test_acc: 0.427\n",
      "epoch: 580 train_acc: 0.584 test_acc: 0.428\n",
      "epoch: 590 train_acc: 0.585 test_acc: 0.429\n",
      "epoch: 600 train_acc: 0.586 test_acc: 0.430\n",
      "epoch: 610 train_acc: 0.588 test_acc: 0.430\n",
      "epoch: 620 train_acc: 0.590 test_acc: 0.432\n",
      "epoch: 630 train_acc: 0.591 test_acc: 0.433\n",
      "epoch: 640 train_acc: 0.593 test_acc: 0.433\n",
      "epoch: 650 train_acc: 0.594 test_acc: 0.434\n",
      "epoch: 660 train_acc: 0.596 test_acc: 0.434\n",
      "epoch: 670 train_acc: 0.597 test_acc: 0.434\n",
      "epoch: 680 train_acc: 0.599 test_acc: 0.434\n",
      "epoch: 690 train_acc: 0.600 test_acc: 0.434\n",
      "epoch: 700 train_acc: 0.602 test_acc: 0.434\n",
      "epoch: 710 train_acc: 0.602 test_acc: 0.435\n",
      "epoch: 720 train_acc: 0.604 test_acc: 0.435\n",
      "epoch: 730 train_acc: 0.604 test_acc: 0.435\n",
      "epoch: 740 train_acc: 0.606 test_acc: 0.436\n",
      "epoch: 750 train_acc: 0.607 test_acc: 0.436\n",
      "epoch: 760 train_acc: 0.608 test_acc: 0.437\n",
      "epoch: 770 train_acc: 0.610 test_acc: 0.437\n",
      "epoch: 780 train_acc: 0.611 test_acc: 0.438\n",
      "epoch: 790 train_acc: 0.612 test_acc: 0.438\n",
      "epoch: 800 train_acc: 0.613 test_acc: 0.439\n",
      "epoch: 810 train_acc: 0.615 test_acc: 0.439\n",
      "epoch: 820 train_acc: 0.616 test_acc: 0.439\n",
      "epoch: 830 train_acc: 0.617 test_acc: 0.440\n",
      "epoch: 840 train_acc: 0.618 test_acc: 0.442\n",
      "epoch: 850 train_acc: 0.619 test_acc: 0.442\n",
      "epoch: 860 train_acc: 0.620 test_acc: 0.442\n",
      "epoch: 870 train_acc: 0.621 test_acc: 0.443\n",
      "epoch: 880 train_acc: 0.622 test_acc: 0.443\n",
      "epoch: 890 train_acc: 0.623 test_acc: 0.444\n",
      "epoch: 900 train_acc: 0.623 test_acc: 0.444\n",
      "epoch: 910 train_acc: 0.624 test_acc: 0.445\n",
      "epoch: 920 train_acc: 0.625 test_acc: 0.446\n",
      "epoch: 930 train_acc: 0.625 test_acc: 0.447\n",
      "epoch: 940 train_acc: 0.626 test_acc: 0.448\n",
      "epoch: 950 train_acc: 0.627 test_acc: 0.450\n",
      "epoch: 960 train_acc: 0.628 test_acc: 0.451\n",
      "epoch: 970 train_acc: 0.629 test_acc: 0.453\n",
      "epoch: 980 train_acc: 0.629 test_acc: 0.454\n",
      "epoch: 990 train_acc: 0.630 test_acc: 0.456\n",
      "epoch: 1000 train_acc: 0.631 test_acc: 0.458\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(0,1001):\n",
    "    sess.run(update, feed_dict = {x:data_x, y_:data_y})\n",
    "    if epoch%10 == 0:\n",
    "        train_acc = sess.run(accuracy, feed_dict={x:data_x, y_:data_y})\n",
    "        test_acc = sess.run(accuracy, feed_dict={x:x_test, y_:y_test})\n",
    "        print (\"epoch: %3d train_acc: %.3f test_acc: %.3f\" % (epoch,train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "(hidden1_size, hidden2_size) = (100, 50)\n",
    "\n",
    "# Set the weight of the first layer - 784*100 \n",
    "# The values to be normalized [0.1,1]\n",
    "W1 = tf.Variable(0.1 * np.random.randn(784, hidden1_size).astype(np.float32))\n",
    "\n",
    "# Set the bais of the first layer - 100*1\n",
    "# The values - 0.1\n",
    "b1 = tf.Variable(tf.constant(0.1, shape=[hidden1_size]))\n",
    "\n",
    "# Set the output of the first layer using ReLU\n",
    "# Output size: [?,100]\n",
    "z1 = tf.nn.relu(tf.matmul(x,W1)+b1)\n",
    "\n",
    "# Set the weight of the second layer - 100*50 \n",
    "# The values to be normalized [0.1,1]\n",
    "W2 = tf.Variable(0.1 * np.random.randn(hidden1_size, hidden2_size).astype(np.float32))\n",
    "\n",
    "# Set the bais of the second layer - 50*1\n",
    "# The values - 0.1\n",
    "b2 = tf.Variable(tf.constant(0.1, shape=[hidden2_size]))\n",
    "\n",
    "# Set the output of the second layer using ReLU\n",
    "# Output size: [?,50]\n",
    "z2 = tf.nn.relu(tf.matmul(z1,W2)+b2)\n",
    "\n",
    "# Set the weight of the output layer - 50*25\n",
    "# The values to be normalized [0.1,1]\n",
    "W3 = tf.Variable(tf.zeros([hidden2_size, num_categories]))\n",
    "\n",
    "# Set the bais of the output layer - 25*1\n",
    "# The values - 0.1\n",
    "b3 = tf.Variable(tf.constant(0.1, shape=[num_categories]))\n",
    "\n",
    "# Set the output using SoftMax\n",
    "# Output size: [?,25]\n",
    "y = tf.nn.softmax(tf.matmul(z2, W3) + b3)\n",
    "#y = 1 / (1.0 + tf.exp(-z2))\n",
    "\n",
    "# Define the loss function - Cross entropy\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "# Define the update function - Gradient descent\n",
    "update = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)\n",
    "\n",
    "# Define accuracy - Percentage of predictions did we get right\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1),tf.argmax(y_, 1)) \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 train_acc: 0.047 test_acc: 0.020\n",
      "epoch:  10 train_acc: 0.109 test_acc: 0.090\n",
      "epoch:  20 train_acc: 0.142 test_acc: 0.147\n",
      "epoch:  30 train_acc: 0.232 test_acc: 0.185\n",
      "epoch:  40 train_acc: 0.275 test_acc: 0.261\n",
      "epoch:  50 train_acc: 0.298 test_acc: 0.254\n",
      "epoch:  60 train_acc: 0.297 test_acc: 0.229\n",
      "epoch:  70 train_acc: 0.310 test_acc: 0.256\n",
      "epoch:  80 train_acc: 0.330 test_acc: 0.270\n",
      "epoch:  90 train_acc: 0.345 test_acc: 0.283\n",
      "epoch: 100 train_acc: 0.360 test_acc: 0.312\n",
      "epoch: 110 train_acc: 0.372 test_acc: 0.319\n",
      "epoch: 120 train_acc: 0.395 test_acc: 0.342\n",
      "epoch: 130 train_acc: 0.403 test_acc: 0.339\n",
      "epoch: 140 train_acc: 0.421 test_acc: 0.358\n",
      "epoch: 150 train_acc: 0.425 test_acc: 0.358\n",
      "epoch: 160 train_acc: 0.440 test_acc: 0.370\n",
      "epoch: 170 train_acc: 0.446 test_acc: 0.377\n",
      "epoch: 180 train_acc: 0.455 test_acc: 0.381\n",
      "epoch: 190 train_acc: 0.457 test_acc: 0.375\n",
      "epoch: 200 train_acc: 0.463 test_acc: 0.384\n",
      "epoch: 210 train_acc: 0.469 test_acc: 0.388\n",
      "epoch: 220 train_acc: 0.475 test_acc: 0.393\n",
      "epoch: 230 train_acc: 0.482 test_acc: 0.399\n",
      "epoch: 240 train_acc: 0.486 test_acc: 0.404\n",
      "epoch: 250 train_acc: 0.489 test_acc: 0.402\n",
      "epoch: 260 train_acc: 0.495 test_acc: 0.405\n",
      "epoch: 270 train_acc: 0.496 test_acc: 0.403\n",
      "epoch: 280 train_acc: 0.501 test_acc: 0.405\n",
      "epoch: 290 train_acc: 0.504 test_acc: 0.406\n",
      "epoch: 300 train_acc: 0.508 test_acc: 0.409\n",
      "epoch: 310 train_acc: 0.511 test_acc: 0.410\n",
      "epoch: 320 train_acc: 0.516 test_acc: 0.410\n",
      "epoch: 330 train_acc: 0.520 test_acc: 0.411\n",
      "epoch: 340 train_acc: 0.524 test_acc: 0.411\n",
      "epoch: 350 train_acc: 0.528 test_acc: 0.412\n",
      "epoch: 360 train_acc: 0.532 test_acc: 0.412\n",
      "epoch: 370 train_acc: 0.535 test_acc: 0.412\n",
      "epoch: 380 train_acc: 0.538 test_acc: 0.411\n",
      "epoch: 390 train_acc: 0.541 test_acc: 0.410\n",
      "epoch: 400 train_acc: 0.543 test_acc: 0.409\n",
      "epoch: 410 train_acc: 0.546 test_acc: 0.409\n",
      "epoch: 420 train_acc: 0.549 test_acc: 0.408\n",
      "epoch: 430 train_acc: 0.551 test_acc: 0.409\n",
      "epoch: 440 train_acc: 0.554 test_acc: 0.408\n",
      "epoch: 450 train_acc: 0.557 test_acc: 0.408\n",
      "epoch: 460 train_acc: 0.559 test_acc: 0.408\n",
      "epoch: 470 train_acc: 0.561 test_acc: 0.407\n",
      "epoch: 480 train_acc: 0.564 test_acc: 0.409\n",
      "epoch: 490 train_acc: 0.566 test_acc: 0.411\n",
      "epoch: 500 train_acc: 0.568 test_acc: 0.413\n",
      "epoch: 510 train_acc: 0.570 test_acc: 0.415\n",
      "epoch: 520 train_acc: 0.572 test_acc: 0.421\n",
      "epoch: 530 train_acc: 0.575 test_acc: 0.422\n",
      "epoch: 540 train_acc: 0.577 test_acc: 0.425\n",
      "epoch: 550 train_acc: 0.579 test_acc: 0.426\n",
      "epoch: 560 train_acc: 0.580 test_acc: 0.426\n",
      "epoch: 570 train_acc: 0.582 test_acc: 0.427\n",
      "epoch: 580 train_acc: 0.584 test_acc: 0.428\n",
      "epoch: 590 train_acc: 0.585 test_acc: 0.429\n",
      "epoch: 600 train_acc: 0.586 test_acc: 0.430\n",
      "epoch: 610 train_acc: 0.588 test_acc: 0.430\n",
      "epoch: 620 train_acc: 0.590 test_acc: 0.432\n",
      "epoch: 630 train_acc: 0.591 test_acc: 0.433\n",
      "epoch: 640 train_acc: 0.593 test_acc: 0.433\n",
      "epoch: 650 train_acc: 0.594 test_acc: 0.434\n",
      "epoch: 660 train_acc: 0.596 test_acc: 0.434\n",
      "epoch: 670 train_acc: 0.597 test_acc: 0.434\n",
      "epoch: 680 train_acc: 0.599 test_acc: 0.434\n",
      "epoch: 690 train_acc: 0.600 test_acc: 0.434\n",
      "epoch: 700 train_acc: 0.602 test_acc: 0.434\n",
      "epoch: 710 train_acc: 0.602 test_acc: 0.435\n",
      "epoch: 720 train_acc: 0.604 test_acc: 0.435\n",
      "epoch: 730 train_acc: 0.604 test_acc: 0.435\n",
      "epoch: 740 train_acc: 0.606 test_acc: 0.436\n",
      "epoch: 750 train_acc: 0.607 test_acc: 0.436\n",
      "epoch: 760 train_acc: 0.608 test_acc: 0.437\n",
      "epoch: 770 train_acc: 0.610 test_acc: 0.437\n",
      "epoch: 780 train_acc: 0.611 test_acc: 0.438\n",
      "epoch: 790 train_acc: 0.612 test_acc: 0.438\n",
      "epoch: 800 train_acc: 0.613 test_acc: 0.439\n",
      "epoch: 810 train_acc: 0.615 test_acc: 0.439\n",
      "epoch: 820 train_acc: 0.616 test_acc: 0.439\n",
      "epoch: 830 train_acc: 0.617 test_acc: 0.440\n",
      "epoch: 840 train_acc: 0.618 test_acc: 0.442\n",
      "epoch: 850 train_acc: 0.619 test_acc: 0.442\n",
      "epoch: 860 train_acc: 0.620 test_acc: 0.442\n",
      "epoch: 870 train_acc: 0.621 test_acc: 0.443\n",
      "epoch: 880 train_acc: 0.622 test_acc: 0.443\n",
      "epoch: 890 train_acc: 0.623 test_acc: 0.444\n",
      "epoch: 900 train_acc: 0.623 test_acc: 0.444\n",
      "epoch: 910 train_acc: 0.624 test_acc: 0.445\n",
      "epoch: 920 train_acc: 0.625 test_acc: 0.446\n",
      "epoch: 930 train_acc: 0.625 test_acc: 0.447\n",
      "epoch: 940 train_acc: 0.626 test_acc: 0.448\n",
      "epoch: 950 train_acc: 0.627 test_acc: 0.450\n",
      "epoch: 960 train_acc: 0.628 test_acc: 0.451\n",
      "epoch: 970 train_acc: 0.629 test_acc: 0.453\n",
      "epoch: 980 train_acc: 0.629 test_acc: 0.454\n",
      "epoch: 990 train_acc: 0.630 test_acc: 0.456\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(0,1000):\n",
    "    sess.run(update, feed_dict = {x:data_x, y_:data_y})\n",
    "    #print(W1.eval(session=sess))\n",
    "    if epoch%10 == 0:\n",
    "        train_acc = sess.run(accuracy, feed_dict={x:data_x, y_:data_y})\n",
    "        test_acc = sess.run(accuracy, feed_dict={x:x_test, y_:y_test})\n",
    "        print (\"epoch: %3d train_acc: %.3f test_acc: %.3f\" % (epoch,train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
