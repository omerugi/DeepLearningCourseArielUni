{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rugio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import data and split to train/test , x/y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 24 classes,the class that has the most data is class 17 with 4.713 precents of the data\n",
      "if the data were equals between all classes it needed to be 4.167 precents of the data\n"
     ]
    }
   ],
   "source": [
    "#there are 24 letters but for the order j get an index\n",
    "num_categories=25 \n",
    "\n",
    "# Importing the data - test and train\n",
    "test = pd.read_csv(\"sign_mnist_test.csv\").to_numpy() \n",
    "train = pd.read_csv(\"sign_mnist_train.csv\").to_numpy()\n",
    "\n",
    "# Splitting test into x & y - x = fetures y = labels \n",
    "# Note: we do OneHot on the y to get it as a vector at the size of the categories\n",
    "# then put's 1 where the right label is and zero in the rest\n",
    "y_test=test[:,0]\n",
    "y_test=keras.utils.to_categorical(y_test, num_categories)\n",
    "x_test=test[:,1:]/255\n",
    "\n",
    "# Splitting data into x & y -> x = fetures y = labels\n",
    "# Note: we do OneHot on the y to get it as a vector at the size of the categories\n",
    "# then put's 1 where the right label is and zero in the rest\n",
    "data_y=train[:,0]\n",
    "data_y=keras.utils.to_categorical(data_y, num_categories)\n",
    "data_x=train[:,1:]/255\n",
    "\n",
    "# Getting the number of feturs\n",
    "# Note: in the case of image each pixel and the toal number is n*n\n",
    "lines,features=data_x.shape\n",
    "\n",
    "biggestClass=np.argmax(np.bincount(train[:,0]))\n",
    "precentOfDataOfBiggestClass=np.max(np.bincount(train[:,0]))*100/lines\n",
    "\n",
    "print(\"there are 24 classes,the class that has the most data is class %2d with %.3f precents of the data\"\n",
    "      %(biggestClass,precentOfDataOfBiggestClass))\n",
    "print(\"if the data were equals between all classes it needed to be %.3f precents of the data\" %(100/24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A \"list like\" object that will hold the calculatins as tensors\n",
    "# Size of x: (?, 784)\n",
    "x = tf.placeholder(tf.float32, [None, features])\n",
    "# Size of y_: (?, 25)\n",
    "y_ = tf.placeholder(tf.float32, [None, num_categories])\n",
    "\n",
    "\n",
    "W = tf.Variable(tf.zeros([features,num_categories]))\n",
    "b = tf.Variable(tf.zeros([num_categories]))\n",
    "\n",
    "# Define activation function - Softmax \n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "# Define the loss function - Cross entropy\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(y_,tf.matmul(x, W) + b) )\n",
    "\n",
    "# Define the update function - Gradient descent\n",
    "update = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "\n",
    "# Define accuracy - Percentage of predictions did we get right\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1),tf.argmax(y_, 1)) \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32)) \n",
    "\n",
    "iteration_axis = np.array([])\n",
    "accuracy_axis = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0  Accuracy: 0.020356944 loss: 3.180026\n",
      "Iteration:  100  Accuracy: 0.46430564 loss: 2.319431\n",
      "Iteration:  200  Accuracy: 0.52663136 loss: 1.95018\n",
      "Iteration:  300  Accuracy: 0.54545456 loss: 1.7357868\n",
      "Iteration:  400  Accuracy: 0.5552147 loss: 1.5893288\n",
      "Iteration:  500  Accuracy: 0.56427777 loss: 1.4794325\n",
      "Iteration:  600  Accuracy: 0.5771054 loss: 1.3919833\n",
      "Iteration:  700  Accuracy: 0.5988567 loss: 1.3196275\n",
      "Iteration:  800  Accuracy: 0.61503065 loss: 1.2581043\n",
      "Iteration:  900  Accuracy: 0.625488 loss: 1.2047335\n",
      "Iteration:  1000  Accuracy: 0.6344116 loss: 1.1577194\n",
      "Iteration:  1100  Accuracy: 0.6413832 loss: 1.1157969\n",
      "Iteration:  1200  Accuracy: 0.648773 loss: 1.0780412\n",
      "Iteration:  1300  Accuracy: 0.6510039 loss: 1.0437541\n",
      "Iteration:  1400  Accuracy: 0.65281653 loss: 1.012396\n",
      "Iteration:  1500  Accuracy: 0.6536531 loss: 0.98354137\n",
      "Iteration:  1600  Accuracy: 0.6542108 loss: 0.9568493\n",
      "Iteration:  1700  Accuracy: 0.6542108 loss: 0.9320424\n",
      "Iteration:  1800  Accuracy: 0.65560514 loss: 0.90889215\n",
      "Iteration:  1900  Accuracy: 0.65699947 loss: 0.8872086\n",
      "Iteration:  2000  Accuracy: 0.657836 loss: 0.86683214\n"
     ]
    }
   ],
   "source": [
    "# Define a session\n",
    "sess = tf.Session()\n",
    "# Init all the vaeiables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range (2001):\n",
    "    for j in range(100):\n",
    "        batch_xs = data_x[j*270:(j+1)*270-1,:]\n",
    "        batch_ys = data_y[j*270:(j+1)*270-1,:]\n",
    "        _, loss_val = sess.run([update, loss],feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    if i%100==0:\n",
    "        iteration_axis = np.append(iteration_axis, [i], axis=0)\n",
    "        accuracy_axis = np.append(accuracy_axis, [sess.run(accuracy, feed_dict={x: x_test, y_: y_test})], axis=0)\n",
    "        print(\"Iteration: \",i, \" Accuracy:\", sess.run(accuracy, feed_dict={x: x_test, y_: y_test}),\"loss:\",loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
