{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 24 classes,the class that has the most data is class 17 with 4.713 precents of the data\n",
      "if the data were equals between all classes it needed to be 4.167 precents of the data\n"
     ]
    }
   ],
   "source": [
    "#there are 24 letters but for the order j get an index\n",
    "num_categories=25 \n",
    "\n",
    "# Importing the data - test and train\n",
    "test = pd.read_csv(\"sign_mnist_test.csv\").to_numpy() \n",
    "train = pd.read_csv(\"sign_mnist_train.csv\").to_numpy()\n",
    "\n",
    "# Splitting test into x & y - x = fetures y = labels \n",
    "# Note: we do OneHot on the y to get it as a vector at the size of the categories\n",
    "# then put's 1 where the right label is and zero in the rest\n",
    "y_test=test[:,0]\n",
    "y_test=keras.utils.to_categorical(y_test, num_categories)\n",
    "x_test=test[:,1:]\n",
    "\n",
    "# Splitting data into x & y - x = fetures y = labels\n",
    "# Note: we do OneHot on the y to get it as a vector at the size of the categories\n",
    "# then put's 1 where the right label is and zero in the rest\n",
    "data_y=train[:,0]\n",
    "data_y=keras.utils.to_categorical(data_y, num_categories)\n",
    "data_x=train[:,1:]\n",
    "\n",
    "# Getting the number of feturs\n",
    "# Note: in the case of image each pixel and the toal number is n*n\n",
    "lines,features=data_x.shape\n",
    "\n",
    "biggestClass=np.argmax(np.bincount(train[:,0]))\n",
    "precentOfDataOfBiggestClass=np.max(np.bincount(train[:,0]))*100/lines\n",
    "\n",
    "print(\"there are 24 classes,the class that has the most data is class %2d with %.3f precents of the data\"\n",
    "      %(biggestClass,precentOfDataOfBiggestClass))\n",
    "print(\"if the data were equals between all classes it needed to be %.3f precents of the data\" %(100/24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A \"list like\" object that will hold the calculatins as tensors\n",
    "# Size of x: (?, 784)\n",
    "x = tf.placeholder(tf.float32, [None, features])\n",
    "# Size of y_: (?, 25)\n",
    "y_ = tf.placeholder(tf.float32, [None, num_categories])\n",
    "\n",
    "(hidden1_size, hidden2_size) = (100, 50)\n",
    "\n",
    "# Set the weight of the first layer - 784*100 \n",
    "# The values to be normalized [0.1,1]\n",
    "W1 = tf.Variable(0.1 * np.random.randn(784, hidden1_size).astype(np.float32))\n",
    "\n",
    "# Set the bais of the first layer - 100*1\n",
    "# The values - 0.1\n",
    "b1 = tf.Variable(tf.constant(0.1, shape=[hidden1_size]))\n",
    "\n",
    "# Set the output of the first layer using ReLU\n",
    "# Output size: [?,100]\n",
    "z1 = tf.nn.relu(tf.matmul(x,W1)+b1)\n",
    "\n",
    "# Set the weight of the second layer - 100*50 \n",
    "# The values to be normalized [0.1,1]\n",
    "W2 = tf.Variable(0.1 * np.random.randn(hidden1_size, hidden2_size).astype(np.float32))\n",
    "\n",
    "# Set the bais of the second layer - 50*1\n",
    "# The values - 0.1\n",
    "b2 = tf.Variable(tf.constant(0.1, shape=[hidden2_size]))\n",
    "\n",
    "# Set the output of the second layer using ReLU\n",
    "# Output size: [?,50]\n",
    "z2 = tf.nn.relu(tf.matmul(z1,W2)+b2)\n",
    "\n",
    "# Set the weight of the output layer - 50*25\n",
    "# The values to be normalized [0.1,1]\n",
    "W3 = tf.Variable(tf.zeros([hidden2_size, num_categories]))\n",
    "\n",
    "# Set the bais of the output layer - 25*1\n",
    "# The values - 0.1\n",
    "b3 = tf.Variable(tf.constant(0.1, shape=[num_categories]))\n",
    "\n",
    "# Set the output using SoftMax\n",
    "# Output size: [?,25]\n",
    "y = tf.nn.softmax(tf.matmul(z2, W3) + b3)\n",
    "#y = 1 / (1.0 + tf.exp(-z2))\n",
    "\n",
    "# Define the loss function - Cross entropy\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "# Define the update function - Gradient descent\n",
    "update = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)\n",
    "\n",
    "# Define accuracy - Percentage of predictions did we get right\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1),tf.argmax(y_, 1)) \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 train_acc: 0.045 test_acc: 0.029\n",
      "epoch:   1 train_acc: 0.083 test_acc: 0.112\n",
      "epoch:   2 train_acc: 0.094 test_acc: 0.090\n",
      "epoch:   3 train_acc: 0.042 test_acc: 0.043\n",
      "epoch:   4 train_acc: 0.057 test_acc: 0.039\n",
      "epoch:   5 train_acc: 0.041 test_acc: 0.046\n",
      "epoch:   6 train_acc: 0.041 test_acc: 0.046\n",
      "epoch:   7 train_acc: 0.041 test_acc: 0.046\n",
      "epoch:   8 train_acc: 0.041 test_acc: 0.046\n",
      "epoch:   9 train_acc: 0.041 test_acc: 0.046\n",
      "epoch:  10 train_acc: 0.041 test_acc: 0.046\n",
      "epoch:  11 train_acc: 0.041 test_acc: 0.046\n",
      "epoch:  12 train_acc: 0.041 test_acc: 0.046\n",
      "epoch:  13 train_acc: 0.041 test_acc: 0.046\n",
      "epoch:  14 train_acc: 0.041 test_acc: 0.046\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(0,15):\n",
    "    sess.run(update, feed_dict = {x:data_x, y_:data_y})\n",
    "    #print(W1.eval(session=sess))\n",
    "    train_acc = sess.run(accuracy, feed_dict={x:data_x, y_:data_y})\n",
    "    test_acc = sess.run(accuracy, feed_dict={x:x_test, y_:y_test})\n",
    "    print (\"epoch: %3d train_acc: %.3f test_acc: %.3f\" % (epoch,train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hidden_layer_nodes = 10\n",
    "\n",
    "W1 = tf.Variable(0.1 * np.random.randn(784, hidden1_size).astype(np.float32))\n",
    "b1 = tf.Variable(tf.constant(0.1, shape=[hidden_layer_nodes]))\n",
    "z1 = tf.add(tf.matmul(x,W1),b1)\n",
    "a1 = tf.nn.relu(z1)\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([hidden_layer_nodes,feasures], stddev=0.1))\n",
    "b2 = tf.Variable(0.)\n",
    "z2 = tf.matmul(a1,W2) + b2\n",
    "y = tf.nn.softmax(z2)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(0,15):\n",
    "    sess.run(update, feed_dict = {x:data_x, y_:data_y})\n",
    "    train_acc = sess.run(accuracy, feed_dict={x:data_x, y_:data_y})\n",
    "    test_acc = sess.run(accuracy, feed_dict={x:x_test, y_:y_test})\n",
    "    print (\"epoch: %3d train_acc: %.3f test_acc: %.3f\" % (epoch,train_acc, test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
